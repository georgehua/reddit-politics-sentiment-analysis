{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Reddit Politics Sentiment Analysis\n",
    "\n",
    "![](Redditpolitics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "This project conducts sentiment analysis to classify a Reddit comment's political affiliation, ranging from Left, Center, Right and Alt. The data is scrapped with [Pushshift API](https://github.com/pushshift/api). The table below is the count of each data file, and the sources of comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s](data_snapshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing\n",
    "\n",
    "The raw comments, as given, are not in a form amenable to feature extraction for classification – there is too much ‘noise’. Therefore, the first step is to clean the text comments, including the process of tagging, lemmatization, and token segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import html\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from print_schema import print_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean each comment, Lemmatization, Tagging & Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(comment):\n",
    "    ''' This function pre-processes a single comment\n",
    "\n",
    "    Parameters:                                                                      \n",
    "        comment : string, the body of a comment\n",
    "\n",
    "    Returns:\n",
    "        modComm : string, the modified comment \n",
    "    '''\n",
    "    modComm = comment\n",
    "    \n",
    "    #modify this to handle other whitespace chars.\n",
    "    #replace newlines with spaces\n",
    "    modComm = re.sub(r\"[\\n\\t\\r]{1,}\", \" \", modComm)\n",
    "        \n",
    "\n",
    "    # unescape html\n",
    "    modComm = html.unescape(modComm)\n",
    "\n",
    "    # remove URLs\n",
    "    modComm = re.sub(r\"(http|www)\\S+\", \"\", modComm)\n",
    "        \n",
    "    #remove duplicate spaces.\n",
    "    modComm = re.sub(' +', ' ', modComm)\n",
    "\n",
    "\n",
    "    # get Spacy document for modComm\n",
    "    utt = nlp(modComm)\n",
    "    # use Spacy document for modComm to create a string.\n",
    "\n",
    "    # Insert \"\\n\" between sentences.\n",
    "    # Split tokens with spaces.\n",
    "    # Write \"/POS\" after each token.\n",
    "    text = \"\"\n",
    "    for sent in utt.sents:\n",
    "\n",
    "        for i, token in enumerate(sent):\n",
    "            first = token.lemma_\n",
    "\n",
    "            # Replace the token itself with the token.lemma . E.g., words/NNS becomes word/NNS.\n",
    "            # If the lemma begins with a dash (‘-’) when the token doesn’t (e.g., -PRON- for I, just keep the token.\n",
    "            if token.lemma_.startswith(\"-\") and not token.text.startswith(\"-\"):\n",
    "                first = token.text\n",
    "\n",
    "            # Retain the case of the original token when you perform this replacement. We make two\n",
    "            # distinctions here: if the original token is entirely in uppercase, the so is the lemma; otherwise,\n",
    "            # keep the lemma in lowercase.\n",
    "            first = first.lower()\n",
    "\n",
    "            if token.text.isupper():\n",
    "               first = first.upper()\n",
    "\n",
    "            second = token.tag_\n",
    "\n",
    "            text += f\"{first}/{second}\"\n",
    "            if i < len(sent) - 1:\n",
    "                text += \" \"\n",
    "\n",
    "        text += \"\\n\"\n",
    "\n",
    "    modComm = text\n",
    "\n",
    "    return modComm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all data files and clean all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\Alt\n",
      "Processing data\\Center\n",
      "Processing data\\Left\n",
      "Processing data\\Right\n"
     ]
    }
   ],
   "source": [
    "allOutput = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"data\"):\n",
    "        \n",
    "    for file in files:    \n",
    "        \n",
    "        fullFile = os.path.join(subdir, file)\n",
    "        print( \"Processing \" + fullFile)\n",
    "\n",
    "        data = json.load(open(fullFile))\n",
    "\n",
    "        # process each line\n",
    "        for line in data:\n",
    "            j = json.loads(line)\n",
    "\n",
    "            # if the comment is deleted, then treat it as an empty string\n",
    "            new_body = j[\"body\"]\n",
    "            if j[\"body\"] == \"[deleted]\" or j[\"body\"] == \"[removed]\":\n",
    "                new_body = \"\"\n",
    "\n",
    "            # clean each comment\n",
    "            new_body = clean_comment(new_body)\n",
    "\n",
    "            # append to final output\n",
    "            new_output = {\n",
    "                \"id\": j[\"id\"],\n",
    "                \"body\": new_body,\n",
    "                \"cat\": file\n",
    "            }\n",
    "\n",
    "            allOutput.append(new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-  list \t - list [120000] <class 'dict'>\n",
      "    |- id\t - <class 'str'>\n",
      "    |- body\t - <class 'str'>\n",
      "    |- cat\t - <class 'str'>\n"
     ]
    }
   ],
   "source": [
    " print_schema(allOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classifying political opinions, I want to extract features that are relevant to bias detection. Several of these features involve counting tokens based on their tags. For example, counting the number of adverbs in a comment involves counting the number of tokens that have been tagged as RB, RBR, or RBS.\n",
    "\n",
    "The features also include norm sets. Lexical norms are aggregate subjective scores given to words by a large group of individuals. Each type of norm assigns a numerical value to each word. [Bristol & GilhoolyLogie's](https://link.springer.com/article/10.3758/BF03201693) set covers age-of-acquisition, imagery, concreteness, familiarity, and ambiguity measures for 1,944 words of varying length and frequency of occurrence are presented. Similarly, [Warringer's](http://crr.ugent.be/archives/1003) norm set. The author collected affective norms of valence (the pleasantness of a stimulus), arousal (the intensity of emotion provoked by a stimulus), and dominance (the degree of control exerted by a stimulus) for 13,915 English words (lemmas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists.\n",
    "FIRST_PERSON_PRONOUNS = {\n",
    "    'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n",
    "SECOND_PERSON_PRONOUNS = {\n",
    "    'you', 'your', 'yours', 'u', 'ur', 'urs'}\n",
    "THIRD_PERSON_PRONOUNS = {\n",
    "    'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them',\n",
    "    'their', 'theirs'}\n",
    "FUTURE_TENSE = {'\\'ll', 'will', 'gonna'}\n",
    "SLANG = {\n",
    "    'smh', 'fwb', 'lmfao', 'lmao', 'lms', 'tbh', 'rofl', 'wtf', 'bff',\n",
    "    'wyd', 'lylc', 'brb', 'atm', 'imao', 'sml', 'btw', 'bw', 'imho', 'fyi',\n",
    "    'ppl', 'sob', 'ttyl', 'imo', 'ltr', 'thx', 'kk', 'omg', 'omfg', 'ttys',\n",
    "    'afn', 'bbs', 'cya', 'ez', 'f2f', 'gtr', 'ic', 'jk', 'k', 'ly', 'ya',\n",
    "    'nm', 'np', 'plz', 'ru', 'so', 'tc', 'tmi', 'ym', 'ur', 'u', 'sol', 'fml'}\n",
    "\n",
    "NORMS_BG = {}\n",
    "NORMS_W = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMultiplePuncToken(token):\n",
    "    ''' Helper funnction to check if a word is multi-character punctuation\n",
    "    Parameters:\n",
    "        token : string, a word\n",
    "    Returns:\n",
    "        boolean : if the word contains multi-chracter puncuation\n",
    "    '''\n",
    "    if len(token) <= 1:\n",
    "        return False\n",
    "\n",
    "    for i in token:\n",
    "        if i not in string.punctuation:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def extract(comment):\n",
    "    ''' This function extracts features from a single comment\n",
    "\n",
    "    Parameters:\n",
    "        comment : string, the body of a comment (after preprocessing)\n",
    "\n",
    "    Returns:\n",
    "        feats : numpy Array, a 29-length vector of floating point features\n",
    "    '''    \n",
    "    # Extract features that rely on capitalization.\n",
    "    # Lowercase the text in comment. Be careful not to lowercase the tags. (e.g. \"Dog/NN\" -> \"dog/NN\").\n",
    "    # Extract features that do not rely on capitalization.\n",
    "\n",
    "    feats = np.zeros((1, 29))\n",
    "\n",
    "    # a list contains all word/tag token\n",
    "    # replace \\n with empty string, then split the comment\n",
    "    word_list = comment.replace(\"\\n\", \" \").split(\" \")[:-1]\n",
    "\n",
    "    # convert to a dictionary for efficient looping\n",
    "    # Format: { '1': ('dog', 'NN'), .... }\n",
    "    word_dict = { i: (t.rsplit(\"/\", 1)[0], t.rsplit(\"/\", 1)[1]) for i, t in enumerate(word_list) if len(t.rsplit(\"/\",1)) == 2}\n",
    "\n",
    "    # start to extract features\n",
    "    for key in word_dict:\n",
    "\n",
    "        word, tag = word_dict[key][0], word_dict[key][1]\n",
    "\n",
    "        # 1. Number of tokens in uppercase (≥ 3 letters long)\n",
    "        if word.isupper() and len(word) >= 3:\n",
    "            feats[0][0] += 1\n",
    "\n",
    "        # change word to lower case\n",
    "        word = word.lower()\n",
    "\n",
    "        # 2. Number of first-person pronouns\n",
    "        if word in FIRST_PERSON_PRONOUNS:\n",
    "            feats[0][1] += 1\n",
    "\n",
    "        # 3. Number of second-person pronouns\n",
    "        if word in SECOND_PERSON_PRONOUNS:\n",
    "            feats[0][2] += 1\n",
    "\n",
    "        # 4. Number of third-person pronouns\n",
    "        if word in THIRD_PERSON_PRONOUNS:\n",
    "            feats[0][3] += 1\n",
    "\n",
    "        # 5. Number of coordinating conjunctions\n",
    "        if tag == 'CC':\n",
    "            feats[0][4] += 1\n",
    "\n",
    "        # 6. Number of past-tense verbs\n",
    "        if tag == 'VBD':\n",
    "            feats[0][5] += 1\n",
    "\n",
    "        # 7. Number of future-tense verbs\n",
    "        if word in FUTURE_TENSE:\n",
    "            feats[0][6] += 1\n",
    "\n",
    "        # 8. Number of commas\n",
    "        if \",\" in word:\n",
    "            feats[0][7] += 1\n",
    "\n",
    "        # 9. Number of multi-character punctuation tokens\n",
    "        # compare first character of the word with punctuation list\n",
    "        if isMultiplePuncToken(word):\n",
    "            feats[0][8] += 1\n",
    "\n",
    "        # 10. Number of common nouns\n",
    "        if tag == \"NN\" or tag == \"NNS\":\n",
    "            feats[0][9] += 1\n",
    "\n",
    "        # 11. Number of proper nouns\n",
    "        if tag == \"NNP\" or tag == \"NNPS\":\n",
    "            feats[0][10] += 1\n",
    "\n",
    "        # 12. Number of adverbs\n",
    "        if tag in [\"RB\", \"RBR\", \"RBS\"]:\n",
    "            feats[0][11] += 1\n",
    "\n",
    "        # 13. Number of wh- words\n",
    "        if tag in ['WDT', 'WP', 'WP$', 'WRB']:\n",
    "            feats[0][12] += 1\n",
    "\n",
    "        # 14. Number of slang acronyms\n",
    "        if word in SLANG:\n",
    "            feats[0][13] += 1\n",
    "\n",
    "\n",
    "    # 7. Number of future-tense verbs\n",
    "    feats[0][6] += len(re.compile(r\"go/VBG to/TO [\\w]+/VB\").findall(comment))\n",
    "\n",
    "    # 15. Average length of sentences, in tokens\n",
    "    feats[0][14] = len(word_list) / comment.count(\"\\n\")\n",
    "\n",
    "    # 16. Average length of tokens, excluding punctuation-only tokens, in characters\n",
    "    f16_i = 0\n",
    "    f16_t = 0\n",
    "    for key in word_dict:\n",
    "\n",
    "        word, tag = word_dict[key][0].lower(), word_dict[key][1]\n",
    "\n",
    "        if word not in string.punctuation and not isMultiplePuncToken(word):\n",
    "            f16_i += 1\n",
    "            f16_t += len(word)\n",
    "\n",
    "    if f16_i > 0:\n",
    "        feats[0][15] = f16_t / f16_i\n",
    "\n",
    "    # 17. Number of sentences.\n",
    "    feats[0][16] = comment.count(\"\\n\")\n",
    "\n",
    "    # Norms features collector\n",
    "    AoA_ls = []\n",
    "    IMG_ls = []\n",
    "    FAM_ls = []\n",
    "\n",
    "    V_ls = []\n",
    "    D_ls = []\n",
    "    A_ls = []\n",
    "\n",
    "    # append norms to their own collection\n",
    "    # for calculating their mean and std\n",
    "    for key in word_dict:\n",
    "\n",
    "        word = word_dict[key][0].lower()\n",
    "\n",
    "        if word != \"\":\n",
    "\n",
    "            # Bristol, Gilhooly, and Logie features\n",
    "            if word in NORMS_BG:\n",
    "                AoA_ls.append(int(NORMS_BG[word][0]))\n",
    "                IMG_ls.append(int(NORMS_BG[word][1]))\n",
    "                FAM_ls.append(int(NORMS_BG[word][2]))\n",
    "\n",
    "            # Warringer features\n",
    "            if word in NORMS_W:\n",
    "                V_ls.append(float(NORMS_W[word][0]))\n",
    "                A_ls.append(float(NORMS_W[word][1]))\n",
    "                D_ls.append(float(NORMS_W[word][2]))\n",
    "\n",
    "\n",
    "    if len(AoA_ls) > 0:\n",
    "        # 18. Average of Ao=pA (100-700) from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][17] = np.mean(AoA_ls)\n",
    "        # 21. Standard deviation of AoA (100-700) from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][20] = np.std(AoA_ls)\n",
    "\n",
    "        # 19. Average of IMG from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][18] = np.mean(IMG_ls)\n",
    "        # 22. Standard deviation of IMG from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][21] = np.std(IMG_ls)\n",
    "\n",
    "        # 20. Average of FAM from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][19] = np.mean(FAM_ls)\n",
    "        # 23. Standard deviation of FAM from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][22] = np.std(FAM_ls)\n",
    "\n",
    "\n",
    "    if len(V_ls) > 0:\n",
    "        # 24. Average of V.Mean.Sum from Warringer norms\n",
    "        feats[0][23] = np.mean(V_ls)\n",
    "        # 27. Standard deviation of V.Mean.Sum from Warringer norms\n",
    "        feats[0][26] = np.std(V_ls)\n",
    "\n",
    "        # 25. Average of A.Mean.Sum from Warringer norms\n",
    "        feats[0][24] = np.mean(A_ls)\n",
    "        # 28. Standard deviation of A.Mean.Sum from Warringer norms\n",
    "        feats[0][27] = np.std(A_ls)\n",
    "\n",
    "        # 26. Average of D.Mean.Sum from Warringer norms\n",
    "        feats[0][25] = np.mean(D_ls)\n",
    "        # 29. Standard deviation of D.Mean.Sum from Warringer norms\n",
    "        feats[0][28] = np.std(D_ls)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "It has been 0.0019998550415039062 seconds since the loop started\n",
      "30000\n",
      "It has been 16.59418511390686 seconds since the loop started\n",
      "60000\n",
      "It has been 32.69585728645325 seconds since the loop started\n",
      "90000\n",
      "It has been 50.40486168861389 seconds since the loop started\n"
     ]
    }
   ],
   "source": [
    "data = allOutput\n",
    "\n",
    "feats = np.zeros((len(data), 30))\n",
    "\n",
    "## fill in containers for norms features\n",
    "\n",
    "# Load Bristol+GilhoolyLogie:\n",
    "bg_file_path = 'Wordlists/BristolNorms+GilhoolyLogie.csv'\n",
    "bg_file = open(bg_file_path, \"r\")\n",
    "reader = csv.reader(bg_file)\n",
    "\n",
    "for i, line in enumerate(reader):\n",
    "    if i > 0:\n",
    "        # dict structure: { word: (AoA, IMG, FAM), ... }\n",
    "        NORMS_BG[line[1]] = (line[3], line[4], line[5])\n",
    "\n",
    "\n",
    "# Load Warringer\n",
    "w_file_path = \"Wordlists//Ratings_Warriner_et_al.csv\"\n",
    "w_file = open(w_file_path, \"r\")\n",
    "reader = csv.reader(w_file)\n",
    "\n",
    "for i, line in enumerate(reader):\n",
    "    if i > 0:\n",
    "        # dict structure: { word: (V, A, D), ... }\n",
    "        NORMS_W[line[1]] = (line[2], line[5], line[8])\n",
    "\n",
    "\n",
    "## extract features\n",
    "loop_starts_time = time.time()\n",
    "for i in range(feats.shape[0]):\n",
    "\n",
    "    body = data[i]['body']\n",
    "    # don't feed into any empty string (usually deleted/removed comment)\n",
    "    if body == \"\":\n",
    "        continue\n",
    "\n",
    "    # Call extract for each datatpoint to find the 29 features.\n",
    "    feats[i][:-1] = extract(body)\n",
    "\n",
    "    class_map = { \"Left\": 0, \"Center\": 1, \"Right\": 2, \"Alt\": 3 }\n",
    "    \n",
    "    # append label in the end\n",
    "    feats[i][-1] = class_map[data[i]['cat']]\n",
    "\n",
    "    if i % 30000 == 0:\n",
    "        print(i)\n",
    "        now = time.time()\n",
    "        print(\"It has been {0} seconds since the loop started\".format(now - loop_starts_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape\n",
    "# np.savez_compressed(\"feats.npz\", feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Classification\n",
    "\n",
    "In this section, I want to build classification models with various machine learning techniques and compare their performance, and pick the best-fit one to tune hyper-parameters and feature engineering. The tested models are as follows:\n",
    "\n",
    "ML models:\n",
    "\n",
    "- SGDClassifier\n",
    "    - Support vector machine (SVM) with stochastic gradient descent estimation\n",
    "- GaussianNB\n",
    "    - Gaussian Navie Bayes, one downside: the assumption that all features are independent is not usually the case in real life so it makes naive bayes algorithm less accurate than complicated algorithms.\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "\n",
    "Neural Network model:\n",
    "\n",
    "- MLP\n",
    "    - Multi-layer perceptra, the most basic implementation of neural network models (ANN)\n",
    "    \n",
    "From the experiments below, we can find MLP is the best model. The model's robust predicting ability render a 0.44 overall accuracy across 4 political groups. And the \"left\" has the best accuracy (0.68) than the \"alt\" (0.32~). The gap between the accuracies may be caused by: 1. \"alt\" has lower support than \"left\" 2. The language used by \"alt\" is more diverse than \"left\", which makes it hard to catch the sentiment.\n",
    "\n",
    "Another finding is after feature selection and hyper-parameter tuning of MLP model, the overall performance didn't have a visible increase. I believe the neural model already reached the top performance based on the existing combination of features. Natural language processing has the nature of complexity and it's hard to obtain a \"perfect model\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feats[..., :-1]  # input (173 features)\n",
    "y = feats[..., -1]  #  label (last column)\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(C):\n",
    "    ''' Compute accuracy given Numpy array confusion matrix C. Returns a floating point value '''\n",
    "\n",
    "    sum, correct = 0.0, 0.0\n",
    "    for i in range(C.shape[0]):  # row i\n",
    "        for j in range(C.shape[1]):  # col j\n",
    "            if i == j:\n",
    "                correct += C[i][j]\n",
    "            sum += C[i][j]\n",
    "\n",
    "    if sum == 0:\n",
    "        return 0.0\n",
    "    return correct / sum\n",
    "\n",
    "def recall(C):\n",
    "    ''' Compute recall given Numpy array confusion matrix C. Returns a list of floating point values '''\n",
    "\n",
    "    TP = np.diag(C)\n",
    "    FP = np.sum(C, axis=0) - TP\n",
    "    FN = np.sum(C, axis=1) - TP\n",
    "\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def precision(C):\n",
    "    ''' Compute precision given Numpy array confusion matrix C. Returns a list of floating point values '''\n",
    "\n",
    "    TP = np.diag(C)\n",
    "    FP = np.sum(C, axis=0) - TP\n",
    "    FN = np.sum(C, axis=1) - TP\n",
    "\n",
    "    return TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SGDClassifier:\n",
      "\n",
      "\tAccuracy: 0.3849\n",
      "\n",
      "\tRecall: [0.6748, 0.2308, 0.2676, 0.2456]\n",
      "\n",
      "\tPrecision: [0.5375, 0.2549, 0.312, 0.2805]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5192  894  757  851]\n",
      " [1426 1241 1212 1498]\n",
      " [1430 1446 1456 1108]\n",
      " [1612 1288 1241 1348]]\n",
      "\n",
      "\n",
      "Results for GaussianNB:\n",
      "\n",
      "\tAccuracy: 0.3476\n",
      "\n",
      "\tRecall: [0.4444, 0.7859, 0.041, 0.0864]\n",
      "\n",
      "\tPrecision: [0.5688, 0.2672, 0.5034, 0.2737]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[3419 3627   65  583]\n",
      " [ 679 4226   73  399]\n",
      " [1094 3847  223  276]\n",
      " [ 819 4114   82  474]]\n",
      "\n",
      "\n",
      "Results for RandomForestClassifier:\n",
      "\n",
      "\tAccuracy: 0.4318\n",
      "\n",
      "\tRecall: [0.6514, 0.3511, 0.5588, 0.0772]\n",
      "\n",
      "\tPrecision: [0.6173, 0.315, 0.3583, 0.302]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5012 1003 1335  344]\n",
      " [1136 1888 1970  383]\n",
      " [ 764 1383 3040  253]\n",
      " [1207 1719 2139  424]]\n",
      "\n",
      "\n",
      "Results for MLPClassifier:\n",
      "\n",
      "\tAccuracy: 0.4427\n",
      "\n",
      "\tRecall: [0.6584, 0.2829, 0.4858, 0.254]\n",
      "\n",
      "\tPrecision: [0.6322, 0.3285, 0.3849, 0.3105]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5066  788  948  892]\n",
      " [1021 1521 1614 1221]\n",
      " [ 756 1058 2643  983]\n",
      " [1170 1263 1662 1394]]\n",
      "\n",
      "\n",
      "Results for AdaBoostClassifier:\n",
      "\n",
      "\tAccuracy: 0.4413\n",
      "\n",
      "\tRecall: [0.6704, 0.2912, 0.5267, 0.1827]\n",
      "\n",
      "\tPrecision: [0.6029, 0.336, 0.3727, 0.324]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5158  783 1120  633]\n",
      " [1191 1566 1769  851]\n",
      " [ 929 1037 2865  609]\n",
      " [1278 1275 1933 1003]]\n",
      "\n",
      "\n",
      "---------------------\n",
      "Best model index number is:  3\n"
     ]
    }
   ],
   "source": [
    "def compare_nude_models(X_train, X_test, y_train, y_test):\n",
    "    ''' \n",
    "    Parameters\n",
    "       X_train: NumPy array, with the selected training features\n",
    "       X_test: NumPy array, with the selected testing features\n",
    "       y_train: NumPy array, with the selected training classes\n",
    "       y_test: NumPy array, with the selected testing classes\n",
    "\n",
    "    Returns:      \n",
    "       i: int, the index of the supposed best classifier\n",
    "    '''\n",
    "\n",
    "    y_true = y_test\n",
    "    results = []\n",
    "    best_accuracy = 0\n",
    "    iBest = 0\n",
    "\n",
    "    # 1. SGDClassifier: support vector machine with a linear kernel.\n",
    "    clf = make_pipeline(StandardScaler(),SGDClassifier())\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_sgd = clf.predict(X_test)\n",
    "    cm1 = confusion_matrix(y_true, y_pred_sgd)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"SGDClassifier\",\n",
    "        \"conf_matrix\": cm1,\n",
    "        \"accuracy\": accuracy(cm1),\n",
    "        \"recall\": recall(cm1),\n",
    "        \"precision\": precision(cm1)\n",
    "    })\n",
    "\n",
    "    # 2. GaussianNB: a Gaussian naive Bayes classifier.\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_gnb = clf.predict(X_test)\n",
    "    cm2 = confusion_matrix(y_true, y_pred_gnb)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"GaussianNB\",\n",
    "        \"conf_matrix\": cm2,\n",
    "        \"accuracy\": accuracy(cm2),\n",
    "        \"recall\": recall(cm2),\n",
    "        \"precision\": precision(cm2)\n",
    "    })\n",
    "\n",
    "    # 3. RandomForestClassifier: with a maximum depth of 5, and 10 estimators.\n",
    "    clf = RandomForestClassifier(max_depth=5, random_state=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_rf = clf.predict(X_test)\n",
    "    cm3 = confusion_matrix(y_true, y_pred_rf)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"RandomForestClassifier\",\n",
    "        \"conf_matrix\": cm3,\n",
    "        \"accuracy\": accuracy(cm3),\n",
    "        \"recall\": recall(cm3),\n",
    "        \"precision\": precision(cm3)\n",
    "    })\n",
    "\n",
    "    # 4. MLPClassifier: A feed-forward neural network, with α = 0.05.\n",
    "    clf = MLPClassifier(alpha=0.05)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_mlp = clf.predict(X_test)\n",
    "    cm4 = confusion_matrix(y_true, y_pred_mlp)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"MLPClassifier\",\n",
    "        \"conf_matrix\": cm4,\n",
    "        \"accuracy\": accuracy(cm4),\n",
    "        \"recall\": recall(cm4),\n",
    "        \"precision\": precision(cm4)\n",
    "    })\n",
    "\n",
    "    # 5. AdaBoostClassifier: with the default hyper-parameters.\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_ada = clf.predict(X_test)\n",
    "    cm5 = confusion_matrix(y_true, y_pred_ada)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"AdaBoostClassifier\",\n",
    "        \"conf_matrix\": cm5,\n",
    "        \"accuracy\": accuracy(cm5),\n",
    "        \"recall\": recall(cm5),\n",
    "        \"precision\": precision(cm5)\n",
    "    })\n",
    "\n",
    "    # For each classifier, compute results and write the following output:\n",
    "    for index, model in enumerate(results):\n",
    "\n",
    "        if model[\"accuracy\"] > best_accuracy:\n",
    "            iBest = index\n",
    "            best_accuracy = model[\"accuracy\"]\n",
    "\n",
    "        print(f'Results for {model[\"classifier_name\"]}:\\n')  # Classifier name\n",
    "        print(f'\\tAccuracy: {model[\"accuracy\"]:.4f}\\n')\n",
    "        print(f'\\tRecall: {[round(item, 4) for item in model[\"recall\"]]}\\n')\n",
    "        print(f'\\tPrecision: {[round(item, 4) for item in model[\"precision\"]]}\\n')\n",
    "        print(f'\\tConfusion Matrix: \\n{model[\"conf_matrix\"]}\\n\\n')\n",
    "    \n",
    "    print(\"---------------------\")\n",
    "    print(\"Best model index number is: \", iBest)\n",
    "\n",
    "compare_nude_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:\n",
      " {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      "0.375 (+/-0.019) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.424 (+/-0.007) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'adam'}\n",
      " \n",
      "0.371 (+/-0.012) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.428 (+/-0.006) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      " \n",
      "0.374 (+/-0.007) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.426 (+/-0.007) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'adam'}\n",
      " \n",
      "0.372 (+/-0.032) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.426 (+/-0.008) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      " \n",
      "0.380 (+/-0.007) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'constant', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.427 (+/-0.007) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'constant', 'clf__solver': 'adam'}\n",
      " \n",
      "0.389 (+/-0.011) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'adaptive', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.427 (+/-0.006) for {'clf__activation': 'logistic', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      " \n",
      "0.381 (+/-0.048) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.432 (+/-0.007) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'adam'}\n",
      " \n",
      "0.376 (+/-0.055) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.434 (+/-0.005) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 50, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      " \n",
      "0.379 (+/-0.053) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.430 (+/-0.004) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'constant', 'clf__solver': 'adam'}\n",
      " \n",
      "0.366 (+/-0.040) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.431 (+/-0.006) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (50, 100, 50), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      " \n",
      "0.363 (+/-0.031) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'constant', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.409 (+/-0.033) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'constant', 'clf__solver': 'adam'}\n",
      " \n",
      "0.369 (+/-0.041) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'adaptive', 'clf__solver': 'sgd'}\n",
      " \n",
      "0.407 (+/-0.027) for {'clf__activation': 'relu', 'clf__alpha': 0.0001, 'clf__hidden_layer_sizes': (100,), 'clf__learning_rate': 'adaptive', 'clf__solver': 'adam'}\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline(steps=[('clf', MLPClassifier(max_iter=100))])\n",
    "\n",
    "search_space = [{'clf__hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "                    'clf__activation': ['logistic', 'relu'],\n",
    "                    'clf__solver': ['sgd', 'adam'],\n",
    "                    'clf__alpha': [0.0001],\n",
    "                    'clf__learning_rate': ['constant','adaptive']}]\n",
    "\n",
    "clf = GridSearchCV(pipe, search_space, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.62      0.64      7694\n",
      "         1.0       0.32      0.30      0.31      5377\n",
      "         2.0       0.40      0.43      0.41      5440\n",
      "         3.0       0.31      0.34      0.32      5489\n",
      "\n",
      "    accuracy                           0.44     24000\n",
      "   macro avg       0.43      0.42      0.42     24000\n",
      "weighted avg       0.45      0.44      0.44     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
