{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Reddit Politics Sentiment Analysis\n",
    "\n",
    "![](Redditpolitics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\n",
    "\n",
    "This project conducts sentiment analysis to classify a Reddit comment's political affiliation, ranging from Left, Center, Right and Alt. The data is scrapped with [Pushshift API](https://github.com/pushshift/api). The table below is the count of each data file, and the sources of comments:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![s](data_snapshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data files are too large to parse, I randomly selected 40,000 comments from each political group, so the total data size is 4 * 40,000 = 120,000 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing\n",
    "\n",
    "The raw comments, as given, are not in a form amenable to feature extraction for classification – there is too much ‘noise’. Therefore, the first step is to clean the text comments, including the process of tagging, lemmatization, and token segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import spacy\n",
    "import html\n",
    "import string\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from print_schema import print_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "sentencizer = nlp.add_pipe(\"sentencizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean each comment, Lemmatization, Tagging & Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(comment):\n",
    "    ''' This function pre-processes a single comment\n",
    "\n",
    "    Parameters:                                                                      \n",
    "        comment : string, the body of a comment\n",
    "\n",
    "    Returns:\n",
    "        modComm : string, the modified comment \n",
    "    '''\n",
    "    modComm = comment\n",
    "    \n",
    "    #modify this to handle other whitespace chars.\n",
    "    #replace newlines with spaces\n",
    "    modComm = re.sub(r\"[\\n\\t\\r]{1,}\", \" \", modComm)\n",
    "        \n",
    "\n",
    "    # unescape html\n",
    "    modComm = html.unescape(modComm)\n",
    "\n",
    "    # remove URLs\n",
    "    modComm = re.sub(r\"(http|www)\\S+\", \"\", modComm)\n",
    "        \n",
    "    #remove duplicate spaces.\n",
    "    modComm = re.sub(' +', ' ', modComm)\n",
    "\n",
    "\n",
    "    # get Spacy document for modComm\n",
    "    utt = nlp(modComm)\n",
    "    # use Spacy document for modComm to create a string.\n",
    "\n",
    "    # Insert \"\\n\" between sentences.\n",
    "    # Split tokens with spaces.\n",
    "    # Write \"/POS\" after each token.\n",
    "    text = \"\"\n",
    "    for sent in utt.sents:\n",
    "\n",
    "        for i, token in enumerate(sent):\n",
    "            first = token.lemma_\n",
    "\n",
    "            # Replace the token itself with the token.lemma . E.g., words/NNS becomes word/NNS.\n",
    "            # If the lemma begins with a dash (‘-’) when the token doesn’t (e.g., -PRON- for I, just keep the token.\n",
    "            if token.lemma_.startswith(\"-\") and not token.text.startswith(\"-\"):\n",
    "                first = token.text\n",
    "\n",
    "            # Retain the case of the original token when you perform this replacement. We make two\n",
    "            # distinctions here: if the original token is entirely in uppercase, the so is the lemma; otherwise,\n",
    "            # keep the lemma in lowercase.\n",
    "            first = first.lower()\n",
    "\n",
    "            if token.text.isupper():\n",
    "               first = first.upper()\n",
    "\n",
    "            second = token.tag_\n",
    "\n",
    "            text += f\"{first}/{second}\"\n",
    "            if i < len(sent) - 1:\n",
    "                text += \" \"\n",
    "\n",
    "        text += \"\\n\"\n",
    "\n",
    "    modComm = text\n",
    "\n",
    "    return modComm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse all data files and clean all comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\Alt\n",
      "Processing data\\Center\n",
      "Processing data\\Left\n",
      "Processing data\\Right\n"
     ]
    }
   ],
   "source": [
    "allOutput = []\n",
    "\n",
    "for subdir, dirs, files in os.walk(\"data\"):\n",
    "        \n",
    "    for file in files:    \n",
    "        \n",
    "        fullFile = os.path.join(subdir, file)\n",
    "        print( \"Processing \" + fullFile)\n",
    "\n",
    "        data = json.load(open(fullFile))\n",
    "\n",
    "        # process each line\n",
    "        for line in data:\n",
    "            j = json.loads(line)\n",
    "\n",
    "            # if the comment is deleted, then treat it as an empty string\n",
    "            new_body = j[\"body\"]\n",
    "            if j[\"body\"] == \"[deleted]\" or j[\"body\"] == \"[removed]\":\n",
    "                new_body = \"\"\n",
    "\n",
    "            # clean each comment\n",
    "            new_body = clean_comment(new_body)\n",
    "\n",
    "            # append to final output\n",
    "            new_output = {\n",
    "                \"id\": j[\"id\"],\n",
    "                \"body\": new_body,\n",
    "                \"cat\": file\n",
    "            }\n",
    "\n",
    "            allOutput.append(new_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-  list \t - list [120000] <class 'dict'>\n",
      "    |- id\t - <class 'str'>\n",
      "    |- body\t - <class 'str'>\n",
      "    |- cat\t - <class 'str'>\n"
     ]
    }
   ],
   "source": [
    " print_schema(allOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classifying political opinions, I want to extract features that are relevant to bias detection. Several of these features involve counting tokens based on their tags. For example, counting the number of adverbs in a comment involves counting the number of tokens that have been tagged as RB, RBR, or RBS.\n",
    "\n",
    "The features also include norm sets. Lexical norms are aggregate subjective scores given to words by a large group of individuals. Each type of norm assigns a numerical value to each word. [Bristol & GilhoolyLogie's](https://link.springer.com/article/10.3758/BF03201693) set covers age-of-acquisition, imagery, concreteness, familiarity, and ambiguity measures for 1,944 words of varying length and frequency of occurrence are presented. Similarly, [Warringer's](http://crr.ugent.be/archives/1003) norm set. The author collected affective norms of valence (the pleasantness of a stimulus), arousal (the intensity of emotion provoked by a stimulus), and dominance (the degree of control exerted by a stimulus) for 13,915 English words (lemmas). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordlists.\n",
    "FIRST_PERSON_PRONOUNS = {\n",
    "    'i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours'}\n",
    "SECOND_PERSON_PRONOUNS = {\n",
    "    'you', 'your', 'yours', 'u', 'ur', 'urs'}\n",
    "THIRD_PERSON_PRONOUNS = {\n",
    "    'he', 'him', 'his', 'she', 'her', 'hers', 'it', 'its', 'they', 'them',\n",
    "    'their', 'theirs'}\n",
    "FUTURE_TENSE = {'\\'ll', 'will', 'gonna'}\n",
    "SLANG = {\n",
    "    'smh', 'fwb', 'lmfao', 'lmao', 'lms', 'tbh', 'rofl', 'wtf', 'bff',\n",
    "    'wyd', 'lylc', 'brb', 'atm', 'imao', 'sml', 'btw', 'bw', 'imho', 'fyi',\n",
    "    'ppl', 'sob', 'ttyl', 'imo', 'ltr', 'thx', 'kk', 'omg', 'omfg', 'ttys',\n",
    "    'afn', 'bbs', 'cya', 'ez', 'f2f', 'gtr', 'ic', 'jk', 'k', 'ly', 'ya',\n",
    "    'nm', 'np', 'plz', 'ru', 'so', 'tc', 'tmi', 'ym', 'ur', 'u', 'sol', 'fml'}\n",
    "\n",
    "NORMS_BG = {}\n",
    "NORMS_W = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isMultiplePuncToken(token):\n",
    "    ''' Helper funnction to check if a word is multi-character punctuation\n",
    "    Parameters:\n",
    "        token : string, a word\n",
    "    Returns:\n",
    "        boolean : if the word contains multi-chracter puncuation\n",
    "    '''\n",
    "    if len(token) <= 1:\n",
    "        return False\n",
    "\n",
    "    for i in token:\n",
    "        if i not in string.punctuation:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "def extract(comment):\n",
    "    ''' This function extracts features from a single comment\n",
    "\n",
    "    Parameters:\n",
    "        comment : string, the body of a comment (after preprocessing)\n",
    "\n",
    "    Returns:\n",
    "        feats : numpy Array, a 29-length vector of floating point features\n",
    "    '''    \n",
    "    # Extract features that rely on capitalization.\n",
    "    # Lowercase the text in comment. Be careful not to lowercase the tags. (e.g. \"Dog/NN\" -> \"dog/NN\").\n",
    "    # Extract features that do not rely on capitalization.\n",
    "\n",
    "    feats = np.zeros((1, 29))\n",
    "\n",
    "    # a list contains all word/tag token\n",
    "    # replace \\n with empty string, then split the comment\n",
    "    word_list = comment.replace(\"\\n\", \" \").split(\" \")[:-1]\n",
    "\n",
    "    # convert to a dictionary for efficient looping\n",
    "    # Format: { '1': ('dog', 'NN'), .... }\n",
    "    word_dict = { i: (t.rsplit(\"/\", 1)[0], t.rsplit(\"/\", 1)[1]) for i, t in enumerate(word_list) if len(t.rsplit(\"/\",1)) == 2}\n",
    "\n",
    "    # start to extract features\n",
    "    for key in word_dict:\n",
    "\n",
    "        word, tag = word_dict[key][0], word_dict[key][1]\n",
    "\n",
    "        # 1. Number of tokens in uppercase (≥ 3 letters long)\n",
    "        if word.isupper() and len(word) >= 3:\n",
    "            feats[0][0] += 1\n",
    "\n",
    "        # change word to lower case\n",
    "        word = word.lower()\n",
    "\n",
    "        # 2. Number of first-person pronouns\n",
    "        if word in FIRST_PERSON_PRONOUNS:\n",
    "            feats[0][1] += 1\n",
    "\n",
    "        # 3. Number of second-person pronouns\n",
    "        if word in SECOND_PERSON_PRONOUNS:\n",
    "            feats[0][2] += 1\n",
    "\n",
    "        # 4. Number of third-person pronouns\n",
    "        if word in THIRD_PERSON_PRONOUNS:\n",
    "            feats[0][3] += 1\n",
    "\n",
    "        # 5. Number of coordinating conjunctions\n",
    "        if tag == 'CC':\n",
    "            feats[0][4] += 1\n",
    "\n",
    "        # 6. Number of past-tense verbs\n",
    "        if tag == 'VBD':\n",
    "            feats[0][5] += 1\n",
    "\n",
    "        # 7. Number of future-tense verbs\n",
    "        if word in FUTURE_TENSE:\n",
    "            feats[0][6] += 1\n",
    "\n",
    "        # 8. Number of commas\n",
    "        if \",\" in word:\n",
    "            feats[0][7] += 1\n",
    "\n",
    "        # 9. Number of multi-character punctuation tokens\n",
    "        # compare first character of the word with punctuation list\n",
    "        if isMultiplePuncToken(word):\n",
    "            feats[0][8] += 1\n",
    "\n",
    "        # 10. Number of common nouns\n",
    "        if tag == \"NN\" or tag == \"NNS\":\n",
    "            feats[0][9] += 1\n",
    "\n",
    "        # 11. Number of proper nouns\n",
    "        if tag == \"NNP\" or tag == \"NNPS\":\n",
    "            feats[0][10] += 1\n",
    "\n",
    "        # 12. Number of adverbs\n",
    "        if tag in [\"RB\", \"RBR\", \"RBS\"]:\n",
    "            feats[0][11] += 1\n",
    "\n",
    "        # 13. Number of wh- words\n",
    "        if tag in ['WDT', 'WP', 'WP$', 'WRB']:\n",
    "            feats[0][12] += 1\n",
    "\n",
    "        # 14. Number of slang acronyms\n",
    "        if word in SLANG:\n",
    "            feats[0][13] += 1\n",
    "\n",
    "\n",
    "    # 7. Number of future-tense verbs\n",
    "    feats[0][6] += len(re.compile(r\"go/VBG to/TO [\\w]+/VB\").findall(comment))\n",
    "\n",
    "    # 15. Average length of sentences, in tokens\n",
    "    feats[0][14] = len(word_list) / comment.count(\"\\n\")\n",
    "\n",
    "    # 16. Average length of tokens, excluding punctuation-only tokens, in characters\n",
    "    f16_i = 0\n",
    "    f16_t = 0\n",
    "    for key in word_dict:\n",
    "\n",
    "        word, tag = word_dict[key][0].lower(), word_dict[key][1]\n",
    "\n",
    "        if word not in string.punctuation and not isMultiplePuncToken(word):\n",
    "            f16_i += 1\n",
    "            f16_t += len(word)\n",
    "\n",
    "    if f16_i > 0:\n",
    "        feats[0][15] = f16_t / f16_i\n",
    "\n",
    "    # 17. Number of sentences.\n",
    "    feats[0][16] = comment.count(\"\\n\")\n",
    "\n",
    "    # Norms features collector\n",
    "    AoA_ls = []\n",
    "    IMG_ls = []\n",
    "    FAM_ls = []\n",
    "\n",
    "    V_ls = []\n",
    "    D_ls = []\n",
    "    A_ls = []\n",
    "\n",
    "    # append norms to their own collection\n",
    "    # for calculating their mean and std\n",
    "    for key in word_dict:\n",
    "\n",
    "        word = word_dict[key][0].lower()\n",
    "\n",
    "        if word != \"\":\n",
    "\n",
    "            # Bristol, Gilhooly, and Logie features\n",
    "            if word in NORMS_BG:\n",
    "                AoA_ls.append(int(NORMS_BG[word][0]))\n",
    "                IMG_ls.append(int(NORMS_BG[word][1]))\n",
    "                FAM_ls.append(int(NORMS_BG[word][2]))\n",
    "\n",
    "            # Warringer features\n",
    "            if word in NORMS_W:\n",
    "                V_ls.append(float(NORMS_W[word][0]))\n",
    "                A_ls.append(float(NORMS_W[word][1]))\n",
    "                D_ls.append(float(NORMS_W[word][2]))\n",
    "\n",
    "\n",
    "    if len(AoA_ls) > 0:\n",
    "        # 18. Average of Ao=pA (100-700) from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][17] = np.mean(AoA_ls)\n",
    "        # 21. Standard deviation of AoA (100-700) from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][20] = np.std(AoA_ls)\n",
    "\n",
    "        # 19. Average of IMG from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][18] = np.mean(IMG_ls)\n",
    "        # 22. Standard deviation of IMG from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][21] = np.std(IMG_ls)\n",
    "\n",
    "        # 20. Average of FAM from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][19] = np.mean(FAM_ls)\n",
    "        # 23. Standard deviation of FAM from Bristol, Gilhooly, and Logie norms\n",
    "        feats[0][22] = np.std(FAM_ls)\n",
    "\n",
    "\n",
    "    if len(V_ls) > 0:\n",
    "        # 24. Average of V.Mean.Sum from Warringer norms\n",
    "        feats[0][23] = np.mean(V_ls)\n",
    "        # 27. Standard deviation of V.Mean.Sum from Warringer norms\n",
    "        feats[0][26] = np.std(V_ls)\n",
    "\n",
    "        # 25. Average of A.Mean.Sum from Warringer norms\n",
    "        feats[0][24] = np.mean(A_ls)\n",
    "        # 28. Standard deviation of A.Mean.Sum from Warringer norms\n",
    "        feats[0][27] = np.std(A_ls)\n",
    "\n",
    "        # 26. Average of D.Mean.Sum from Warringer norms\n",
    "        feats[0][25] = np.mean(D_ls)\n",
    "        # 29. Standard deviation of D.Mean.Sum from Warringer norms\n",
    "        feats[0][28] = np.std(D_ls)\n",
    "\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "It has been 0.0010001659393310547 seconds since the loop started\n",
      "30000\n",
      "It has been 16.621175050735474 seconds since the loop started\n",
      "60000\n",
      "It has been 33.30032801628113 seconds since the loop started\n",
      "90000\n",
      "It has been 53.08335471153259 seconds since the loop started\n"
     ]
    }
   ],
   "source": [
    "data = allOutput\n",
    "\n",
    "feats = np.zeros((len(data), 30))\n",
    "\n",
    "## fill in containers for norms features\n",
    "\n",
    "# Load Bristol+GilhoolyLogie:\n",
    "bg_file_path = 'Wordlists/BristolNorms+GilhoolyLogie.csv'\n",
    "bg_file = open(bg_file_path, \"r\")\n",
    "reader = csv.reader(bg_file)\n",
    "\n",
    "for i, line in enumerate(reader):\n",
    "    if i > 0:\n",
    "        # dict structure: { word: (AoA, IMG, FAM), ... }\n",
    "        NORMS_BG[line[1]] = (line[3], line[4], line[5])\n",
    "\n",
    "\n",
    "# Load Warringer\n",
    "w_file_path = \"Wordlists//Ratings_Warriner_et_al.csv\"\n",
    "w_file = open(w_file_path, \"r\")\n",
    "reader = csv.reader(w_file)\n",
    "\n",
    "for i, line in enumerate(reader):\n",
    "    if i > 0:\n",
    "        # dict structure: { word: (V, A, D), ... }\n",
    "        NORMS_W[line[1]] = (line[2], line[5], line[8])\n",
    "\n",
    "\n",
    "## extract features\n",
    "loop_starts_time = time.time()\n",
    "for i in range(feats.shape[0]):\n",
    "\n",
    "    body = data[i]['body']\n",
    "    # don't feed into any empty string (usually deleted/removed comment)\n",
    "    if body == \"\":\n",
    "        continue\n",
    "\n",
    "    # Call extract for each datatpoint to find the 29 features.\n",
    "    feats[i][:-1] = extract(body)\n",
    "\n",
    "    class_map = { \"Left\": 0, \"Center\": 1, \"Right\": 2, \"Alt\": 3 }\n",
    "    \n",
    "    # append label in the end\n",
    "    feats[i][-1] = class_map[data[i]['cat']]\n",
    "\n",
    "    if i % 30000 == 0:\n",
    "        print(i)\n",
    "        now = time.time()\n",
    "        print(\"It has been {0} seconds since the loop started\".format(now - loop_starts_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats.shape\n",
    "# np.savez_compressed(\"feats.npz\", feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Classification\n",
    "\n",
    "In this section, I want to build classification models with various machine learning techniques and compare their performance, then select the best-fit one for this problem. The selected models are as follows:\n",
    "\n",
    "ML models:\n",
    "\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "    - something\n",
    "- GaussianNB\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "\n",
    "Neural Network model:\n",
    "\n",
    "- MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_rel\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import SGDClassifier  \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feats[..., :-1]  # input (173 features)\n",
    "y = feats[..., -1]  #  label (last column)\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(C):\n",
    "    ''' Compute accuracy given Numpy array confusion matrix C. Returns a floating point value '''\n",
    "\n",
    "    sum, correct = 0.0, 0.0\n",
    "    for i in range(C.shape[0]):  # row i\n",
    "        for j in range(C.shape[1]):  # col j\n",
    "            if i == j:\n",
    "                correct += C[i][j]\n",
    "            sum += C[i][j]\n",
    "\n",
    "    if sum == 0:\n",
    "        return 0.0\n",
    "    return correct / sum\n",
    "\n",
    "def recall(C):\n",
    "    ''' Compute recall given Numpy array confusion matrix C. Returns a list of floating point values '''\n",
    "\n",
    "    TP = np.diag(C)\n",
    "    FP = np.sum(C, axis=0) - TP\n",
    "    FN = np.sum(C, axis=1) - TP\n",
    "\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "def precision(C):\n",
    "    ''' Compute precision given Numpy array confusion matrix C. Returns a list of floating point values '''\n",
    "\n",
    "    TP = np.diag(C)\n",
    "    FP = np.sum(C, axis=0) - TP\n",
    "    FN = np.sum(C, axis=1) - TP\n",
    "\n",
    "    return TP/(TP+FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\George\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:585: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SGDClassifier:\n",
      "\n",
      "\tAccuracy: 0.3859\n",
      "\n",
      "\tRecall: [0.6145, 0.3786, 0.1652, 0.2866]\n",
      "\n",
      "\tPrecision: [0.5701, 0.2867, 0.3453, 0.2633]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[4767 1481  451 1059]\n",
      " [1151 2035  628 1561]\n",
      " [1180 1551  891 1771]\n",
      " [1264 2031  610 1569]]\n",
      "\n",
      "\n",
      "Results for GaussianNB:\n",
      "\n",
      "\tAccuracy: 0.3480\n",
      "\n",
      "\tRecall: [0.4395, 0.8067, 0.0421, 0.0691]\n",
      "\n",
      "\tPrecision: [0.575, 0.2658, 0.5147, 0.287]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[3410 3894   48  406]\n",
      " [ 667 4336   61  311]\n",
      " [1010 3934  227  222]\n",
      " [ 843 4148  105  378]]\n",
      "\n",
      "\n",
      "Results for RandomForestClassifier:\n",
      "\n",
      "\tAccuracy: 0.4338\n",
      "\n",
      "\tRecall: [0.6607, 0.3174, 0.5591, 0.103]\n",
      "\n",
      "\tPrecision: [0.6134, 0.3223, 0.3542, 0.307]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5126  883 1305  444]\n",
      " [1166 1706 2033  470]\n",
      " [ 790 1229 3015  359]\n",
      " [1275 1476 2159  564]]\n",
      "\n",
      "\n",
      "Results for MLPClassifier:\n",
      "\n",
      "\tAccuracy: 0.4361\n",
      "\n",
      "\tRecall: [0.739, 0.3033, 0.5083, 0.0661]\n",
      "\n",
      "\tPrecision: [0.5464, 0.3179, 0.3714, 0.3624]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5733  828 1029  168]\n",
      " [1710 1630 1786  249]\n",
      " [1279 1153 2741  220]\n",
      " [1770 1517 1825  362]]\n",
      "\n",
      "\n",
      "Results for AdaBoostClassifier:\n",
      "\n",
      "\tAccuracy: 0.4369\n",
      "\n",
      "\tRecall: [0.6691, 0.2954, 0.4981, 0.1863]\n",
      "\n",
      "\tPrecision: [0.6129, 0.3234, 0.3616, 0.3197]\n",
      "\n",
      "\tConfusion Matrix: \n",
      "[[5191  868 1098  601]\n",
      " [1174 1588 1772  841]\n",
      " [ 843 1136 2686  728]\n",
      " [1262 1319 1873 1020]]\n",
      "\n",
      "\n",
      "---------------------\n",
      "Best model index number is:  4\n"
     ]
    }
   ],
   "source": [
    "def compare_nude_models(X_train, X_test, y_train, y_test):\n",
    "    ''' \n",
    "    Parameters\n",
    "       X_train: NumPy array, with the selected training features\n",
    "       X_test: NumPy array, with the selected testing features\n",
    "       y_train: NumPy array, with the selected training classes\n",
    "       y_test: NumPy array, with the selected testing classes\n",
    "\n",
    "    Returns:      \n",
    "       i: int, the index of the supposed best classifier\n",
    "    '''\n",
    "\n",
    "    y_true = y_test\n",
    "    results = []\n",
    "    best_accuracy = 0\n",
    "    iBest = 0\n",
    "\n",
    "    # 1. SGDClassifier: support vector machine with a linear kernel.\n",
    "    clf = make_pipeline(StandardScaler(),SGDClassifier())\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_sgd = clf.predict(X_test)\n",
    "    cm1 = confusion_matrix(y_true, y_pred_sgd)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"SGDClassifier\",\n",
    "        \"conf_matrix\": cm1,\n",
    "        \"accuracy\": accuracy(cm1),\n",
    "        \"recall\": recall(cm1),\n",
    "        \"precision\": precision(cm1)\n",
    "    })\n",
    "\n",
    "    # 2. GaussianNB: a Gaussian naive Bayes classifier.\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_gnb = clf.predict(X_test)\n",
    "    cm2 = confusion_matrix(y_true, y_pred_gnb)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"GaussianNB\",\n",
    "        \"conf_matrix\": cm2,\n",
    "        \"accuracy\": accuracy(cm2),\n",
    "        \"recall\": recall(cm2),\n",
    "        \"precision\": precision(cm2)\n",
    "    })\n",
    "\n",
    "    # 3. RandomForestClassifier: with a maximum depth of 5, and 10 estimators.\n",
    "    clf = RandomForestClassifier(max_depth=5, random_state=10)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_rf = clf.predict(X_test)\n",
    "    cm3 = confusion_matrix(y_true, y_pred_rf)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"RandomForestClassifier\",\n",
    "        \"conf_matrix\": cm3,\n",
    "        \"accuracy\": accuracy(cm3),\n",
    "        \"recall\": recall(cm3),\n",
    "        \"precision\": precision(cm3)\n",
    "    })\n",
    "\n",
    "    # 4. MLPClassifier: A feed-forward neural network, with α = 0.05.\n",
    "    clf = MLPClassifier(alpha=0.05)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_mlp = clf.predict(X_test)\n",
    "    cm4 = confusion_matrix(y_true, y_pred_mlp)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"MLPClassifier\",\n",
    "        \"conf_matrix\": cm4,\n",
    "        \"accuracy\": accuracy(cm4),\n",
    "        \"recall\": recall(cm4),\n",
    "        \"precision\": precision(cm4)\n",
    "    })\n",
    "\n",
    "    # 5. AdaBoostClassifier: with the default hyper-parameters.\n",
    "    clf = AdaBoostClassifier()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred_ada = clf.predict(X_test)\n",
    "    cm5 = confusion_matrix(y_true, y_pred_ada)\n",
    "    results.append({\n",
    "        \"classifier_name\": \"AdaBoostClassifier\",\n",
    "        \"conf_matrix\": cm5,\n",
    "        \"accuracy\": accuracy(cm5),\n",
    "        \"recall\": recall(cm5),\n",
    "        \"precision\": precision(cm5)\n",
    "    })\n",
    "\n",
    "    # For each classifier, compute results and write the following output:\n",
    "    for index, model in enumerate(results):\n",
    "\n",
    "        if model[\"accuracy\"] > best_accuracy:\n",
    "            iBest = index\n",
    "            best_accuracy = model[\"accuracy\"]\n",
    "\n",
    "        print(f'Results for {model[\"classifier_name\"]}:\\n')  # Classifier name\n",
    "        print(f'\\tAccuracy: {model[\"accuracy\"]:.4f}\\n')\n",
    "        print(f'\\tRecall: {[round(item, 4) for item in model[\"recall\"]]}\\n')\n",
    "        print(f'\\tPrecision: {[round(item, 4) for item in model[\"precision\"]]}\\n')\n",
    "        print(f'\\tConfusion Matrix: \\n{model[\"conf_matrix\"]}\\n\\n')\n",
    "    \n",
    "    print(\"---------------------\")\n",
    "    print(\"Best model index number is: \", iBest)\n",
    "\n",
    "compare_nude_models(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "pipe = Pipeline(steps=[('selector', SelectKBest(f_classif, k=10)), ('clf', MLPClassifier(max_iter=100))])\n",
    "\n",
    "search_space = [{'selector__k': [10, 15]},\n",
    "                {'clf__hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "                    'clf__activation': ['tanh', 'relu'],\n",
    "                    'clf__solver': ['sgd', 'adam'],\n",
    "                    'clf__alpha': [0.0001, 0.05],\n",
    "                    'clf__learning_rate': ['constant','adaptive']}]\n",
    "\n",
    "clf = RandomizedSearchCV(pipe, search_space, n_jobs=-1, cv=3, verbose=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Best paramete set\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "# All results\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_grid = {\n",
    "#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "#     'activation': ['tanh', 'relu'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.05],\n",
    "#     'learning_rate': ['constant','adaptive'],\n",
    "# }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results on the test set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.57      0.72      0.64      7758\n",
      "         1.0       0.33      0.21      0.26      5375\n",
      "         2.0       0.40      0.39      0.39      5393\n",
      "         3.0       0.31      0.32      0.31      5474\n",
      "\n",
      "    accuracy                           0.44     24000\n",
      "   macro avg       0.40      0.41      0.40     24000\n",
      "weighted avg       0.42      0.44      0.42     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = y_test , clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('Results on the test set:')\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
